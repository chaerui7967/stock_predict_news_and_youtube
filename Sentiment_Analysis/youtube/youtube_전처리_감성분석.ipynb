{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "youtube_전처리_감성분석.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kfPR1TZan9tE",
        "MywISYhUr8JZ",
        "hiF2nB7MsH5a"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaerui7967/stock_predict_news_and_youtube/blob/master/youtube_%EC%A0%84%EC%B2%98%EB%A6%AC_%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC8EG61Du_li",
        "outputId": "56934d3e-4014-456c-dd52-0bbaa123073e"
      },
      "source": [
        "!pip install konlpy "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 29.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfPR1TZan9tE"
      },
      "source": [
        "## youtube 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrpzJB1kcHq9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA2ZBOE1bu1B",
        "outputId": "cc01007b-ffcf-4d48-b639-84b0ead1d9bc"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNrfjZ7jdmE1"
      },
      "source": [
        "def set_data(df,com_num = 0):  # panda, numpy, datetime, FinanceDataReader, konlpy, Counter\n",
        "    # 일, 시간 나누기\n",
        "    df['date'] = df['date'].astype(str)\n",
        "    df['date_d'] = df['date'].str[:-2]\n",
        "    df['date_h'] = df['date'].str[-2:]\n",
        "    # 타입을 데이트 타입으로 만듬\n",
        "    df['date_d'] = pd.to_datetime(df['date_d'])\n",
        "    \n",
        "    df = df.sort_values(by='date_d') # 일기준으로 오름차순 정렬\n",
        "    \n",
        "    # 널값 제거\n",
        "    df = df.dropna()\n",
        "    \n",
        "    # 주말 및 공휴일 데이터\n",
        "    ### Holidays\n",
        "    try:\n",
        "        # 서버가 열려있을 때\n",
        "        db = pymysql.connect(user='root',\n",
        "                             passwd='1234',\n",
        "                             host='3.35.70.166',\n",
        "                             db='proj',\n",
        "                             charset='utf8')\n",
        "\n",
        "        cursor = db.cursor(pymysql.cursors.DictCursor)\n",
        "\n",
        "        sql = \"select * from holidays\"\n",
        "        cursor.execute(sql)\n",
        "        result = cursor.fetchall()\n",
        "    \n",
        "        # DataFrame으로 변경\n",
        "        holi = pd.DataFrame(result)\n",
        "        # db 닫기 --> 안하면 메모리 잡아먹음\n",
        "        db.close()\n",
        "    except:\n",
        "        # 서버 없을 때 깃허브에서 바로 가져옴\n",
        "        db_holi = 'https://raw.githubusercontent.com/chaerui7967/stock_predict_news_and_youtube/master/Sentiment_Analysis/data/holidays.csv'\n",
        "        holi = pd.read_csv(db_holi)\n",
        "    \n",
        "    # date 컬럼을 날짜 형식으로 변경\n",
        "    holi['date'] = pd.to_datetime(holi['date'])\n",
        "    \n",
        "    \n",
        "    ### ===================주말 및 공휴일 제외\n",
        "    \n",
        "    ## 주말 및 공휴일만 추출\n",
        "    market_closed = holi[holi['holiday']==\"O\"].reset_index(drop=True)\n",
        "    \n",
        "    ## 휴장일 List 생성\n",
        "    market_closed_list = list(market_closed['date'])\n",
        "    \n",
        "    # 주말 및 공휴일 제외\n",
        "    while len(df[df['date_d'].isin(market_closed_list)]['date_d']) !=0:\n",
        "        for date in df[df['date_d'].isin(market_closed_list)]['date_d'].index:\n",
        "            df['date_d'][date] += datetime.timedelta(1)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mspQelklb9Gf"
      },
      "source": [
        "youtube_df = pd.read_csv('/content/drive/My Drive/Final PJT - 업빛투/감성분석/data/youtube_total.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osNxCzq2e2PZ",
        "outputId": "d0d81cba-ed61-4ac8-99a4-88975175f002"
      },
      "source": [
        "youtube_df = set_data(youtube_df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PWceOAyeVJ7"
      },
      "source": [
        "youtube_df.drop(['date','length', 'url', 'date_h','description'], 1, inplace=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILX6Xw_cc879"
      },
      "source": [
        "youtube_df = youtube_df[['st_n', 'st_cd', 'ch_nm', 'date_d', 'title', 'text', 'views']]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoF9w1H-fpa-"
      },
      "source": [
        "youtube_df.columns = ['st_n', 'st_cd', 'ch_nm', 'date', 'title', 'text', 'views']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "UfDlWW1df6NM",
        "outputId": "66b39256-5da9-4e1e-cfa4-f95723d68d9c"
      },
      "source": [
        "youtube_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>st_n</th>\n",
              "      <th>st_cd</th>\n",
              "      <th>ch_nm</th>\n",
              "      <th>date</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>views</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1362</th>\n",
              "      <td>삼성전자</td>\n",
              "      <td>5930</td>\n",
              "      <td>한국경제TV</td>\n",
              "      <td>2012-06-26</td>\n",
              "      <td>[마감시황] 삼성전자 `흔들` 코스피도 `휘청`‥1820선 하락마감</td>\n",
              "      <td>매우 늘 증시 마감 상황 정리 해보겠습니다 증권 팀 지수 이 기자나와있습니다네 오늘...</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>하이닉스</td>\n",
              "      <td>660</td>\n",
              "      <td>한국경제TV</td>\n",
              "      <td>2012-07-02</td>\n",
              "      <td>SK하이닉스, M12라인 가동  차세대 메모리 본격 강화</td>\n",
              "      <td>sk 하이닉스가 차세대 메모리 반도체 사업 확장에 본격적으로 돌입했습니다청주에 새로...</td>\n",
              "      <td>154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016</th>\n",
              "      <td>LG화학</td>\n",
              "      <td>51910</td>\n",
              "      <td>한국경제TV</td>\n",
              "      <td>2012-07-14</td>\n",
              "      <td>6인의 마감전략   성공투자 오후증시2012년 07월 13일 방송</td>\n",
              "      <td>tweet4 7월 13일 금 일 장 마감 42분 전 이군요 자 6명의 전문가는어떤 ...</td>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1283</th>\n",
              "      <td>삼성전자</td>\n",
              "      <td>5930</td>\n",
              "      <td>한국경제TV</td>\n",
              "      <td>2012-07-30</td>\n",
              "      <td>삼성전자 또 사상최대 실적  2분기 영업이익 6조7천억원</td>\n",
              "      <td>소식입니다 삼성전자가 또다시 사상 최대 실적을 경신했습니다 갤럭시 시리즈의 위력을 ...</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1259</th>\n",
              "      <td>삼성전자</td>\n",
              "      <td>5930</td>\n",
              "      <td>한국경제TV</td>\n",
              "      <td>2012-09-13</td>\n",
              "      <td>삼성전자, 반도체 중국생산 시대 연다</td>\n",
              "      <td>삼성전자가 중국 산시성 시안 시에서 낸드플래시 반도체 공장 기공식을열었습니다삼성전자...</td>\n",
              "      <td>164</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      st_n  st_cd  ...                                               text views\n",
              "1362  삼성전자   5930  ...  매우 늘 증시 마감 상황 정리 해보겠습니다 증권 팀 지수 이 기자나와있습니다네 오늘...    30\n",
              "60    하이닉스    660  ...  sk 하이닉스가 차세대 메모리 반도체 사업 확장에 본격적으로 돌입했습니다청주에 새로...   154\n",
              "2016  LG화학  51910  ...  tweet4 7월 13일 금 일 장 마감 42분 전 이군요 자 6명의 전문가는어떤 ...   115\n",
              "1283  삼성전자   5930  ...  소식입니다 삼성전자가 또다시 사상 최대 실적을 경신했습니다 갤럭시 시리즈의 위력을 ...    36\n",
              "1259  삼성전자   5930  ...  삼성전자가 중국 산시성 시안 시에서 낸드플래시 반도체 공장 기공식을열었습니다삼성전자...   164\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M3gHj6sf8PW"
      },
      "source": [
        "youtube_df.to_csv('./youtube_preprocessing.csv', index=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icQ5m5Wln5c7"
      },
      "source": [
        "## youtube 감성분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eThqqlVkn4uk"
      },
      "source": [
        "youtube_df = pd.read_csv('/content/drive/My Drive/Final PJT - 업빛투/감성분석/data/youtube_preprocessing.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPkEWThSqUIq"
      },
      "source": [
        "aa = youtube_df.text.iloc[:].values"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6GLMkz4oHQ9"
      },
      "source": [
        "## 감성분석 연습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_oYJ48dgkjI"
      },
      "source": [
        "import os\n",
        "\n",
        "def read_data(filename):\n",
        "    with open(filename, 'r', encoding=\"cp949\") as f:\n",
        "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "        data = data[1:]\n",
        "    return data\n",
        "\n",
        "data = read_data(r'/content/drive/My Drive/Colab Notebooks/datasets/ratings_morphed.txt')\n",
        "data_text = [line[1] for line in data]    \n",
        "data_senti = [line[2] for line in data]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0z-4JXKoa37"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data_text, test_data_text, train_data_senti, test_data_senti \\\n",
        "    = train_test_split(\n",
        "        data_text,\n",
        "        data_senti,\n",
        "        stratify=data_senti,\n",
        "        test_size=0.3,\n",
        "        random_state=156\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ViHRgmXofAq",
        "outputId": "136e33c9-8db0-45ce-bbe3-9220b2d89e33"
      },
      "source": [
        "from collections import Counter\n",
        "train_data_senti_freq = Counter(train_data_senti)\n",
        "print('train_data_senti_freq:', train_data_senti_freq)\n",
        "test_data_senti_freq = Counter(test_data_senti)\n",
        "print('test_data_senti_freq:', test_data_senti_freq)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data_senti_freq: Counter({'1': 69158, '0': 69120})\n",
            "test_data_senti_freq: Counter({'1': 29639, '0': 29624})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6eqg_xFog6_",
        "outputId": "657752b2-e6ef-4652-aedf-a1a079fdb962"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer(min_df=5).fit(train_data_text)\n",
        "X_train = vect.transform(train_data_text)\n",
        "\n",
        "feature_names = vect.get_feature_names()\n",
        "print(\"특성 개수:\", len(feature_names))\n",
        "print(\"처음 20개 특성:\\m\", feature_names[:20])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "특성 개수: 11267\n",
            "처음 20개 특성:\\m ['10점', '1빠', 'cgv', 'ebs', 'kbs', 'la', 'mb', 'mbc', 'naver', 'ok', 'sbs', 'sns', 'tv', 'usb', 'ㄴㄴ', 'ㄷㄷ', 'ㅂㅅ', 'ㅅㅂ', 'ㅇㅇ', 'ㅇㅇㅇ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUWuTU-tolIz"
      },
      "source": [
        "# # Tfidf로 벡터화\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# vect = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(train_data_text)\n",
        "# X_train = vect.transform(train_data_text)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyKMPaaYossX",
        "outputId": "8a42b0ab-6d65-4c4c-d2f9-6e0cb9914b44"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "y_train = pd.Series(train_data_senti)\n",
        "scores = cross_val_score(LogisticRegression(solver=\"liblinear\"), X_train, y_train, cv=5)\n",
        "print('교차 검증 점수:', scores)\n",
        "print('교차 검증 점수 평균:', scores.mean())\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'C': [0.01, 0.1, 1, 3, 5]}\n",
        "grid = GridSearchCV(LogisticRegression(solver=\"liblinear\"), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"최고 교차 검증 점수:\", round(grid.best_score_, 3))\n",
        "print(\"최적의 매개변수:\", grid.best_params_)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "교차 검증 점수: [0.82202777 0.81667631 0.81913509 0.81518713 0.81612728]\n",
            "교차 검증 점수 평균: 0.8178307153197114\n",
            "최고 교차 검증 점수: 0.818\n",
            "최적의 매개변수: {'C': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfElnLEooxpY",
        "outputId": "9843ee71-794e-47ab-9793-0b8a150a5ee0"
      },
      "source": [
        "X_test = vect.transform(test_data_text)\n",
        "y_test = pd.Series(test_data_senti)\n",
        "print(\"테스트 데이터 점수:\", grid.score(X_test, y_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 데이터 점수: 0.8206131987918263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-hkpB8bqFR8"
      },
      "source": [
        "predic = vect.transform(aa)\n",
        "pred = grid.predict(predic)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MywISYhUr8JZ"
      },
      "source": [
        "### 감성사전 rhinoMorph 사용__ 그냥 다긍정으로 판단"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu_jCQpPpG8T",
        "outputId": "ce215061-8a40-40a2-bb57-c2f716672914"
      },
      "source": [
        "!pip install rhinoMorph\n",
        "!pip install JPype1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rhinoMorph\n",
            "  Downloading rhinoMorph-3.8.0.0-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 12.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: rhinoMorph\n",
            "Successfully installed rhinoMorph-3.8.0.0\n",
            "Requirement already satisfied: JPype1 in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R6EtYZ-o_1c",
        "outputId": "062055d3-3d18-4016-a657-c7b5dfc1c3a0"
      },
      "source": [
        "import rhinoMorph\n",
        "rn = rhinoMorph.startRhino()\n",
        "print('rn\\n',rn)\n",
        "new_input = '재미 더럽 없'\n",
        "\n",
        "# 입력 데이터 형태소 분석하기\n",
        "inputdata = []\n",
        "morphed_input = rhinoMorph.onlyMorph_list(rn, new_input, pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'])\n",
        "morphed_input = ' '.join(morphed_input)                     # 한 개의 문자열로 만들기\n",
        "inputdata.append(morphed_input)                               # 분석 결과를 리스트로 만들기\n",
        "X_input = vect.transform(inputdata)\n",
        "print(float(grid.predict(X_input)))\n",
        "result = grid.predict(X_input) # 0은 부정,1은 긍정\n",
        "print(result)\n",
        "print(type(result))\n",
        "if result == 0.0:\n",
        "    print(\"부정적인 글입니다\")\n",
        "else:\n",
        "    print(\"긍정적인 글입니다\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filepath:  /usr/local/lib/python3.7/dist-packages\n",
            "classpath:  /usr/local/lib/python3.7/dist-packages/rhinoMorph/lib/rhino.jar\n",
            "RHINO started!\n",
            "rn\n",
            " <java class 'rhino.RHINO'>\n",
            "0.0\n",
            "['0']\n",
            "<class 'numpy.ndarray'>\n",
            "긍정적인 글입니다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiF2nB7MsH5a"
      },
      "source": [
        "### 레이블링_ 긍정비율이 높음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISfJ0bl5prVc"
      },
      "source": [
        "labling = youtube_df[['date','text']]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zslpZ8ULsayu",
        "outputId": "248d2e45-ae84-4ebc-dc7e-92d702b3b457"
      },
      "source": [
        "labling['label'] = pred"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw9ujUZWseY4",
        "outputId": "1bd1c8a0-645b-406b-884c-221b7ca6fac8"
      },
      "source": [
        "labling.label.value_counts()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    5816\n",
              "0     547\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmctuvtAtkWf"
      },
      "source": [
        "## 네이버 리뷰 자료로 감성 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00uquFiPu6Q5"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6Jl-xpttj03",
        "outputId": "7db38bef-21d7-45ca-c8ff-7e6b55d32e29"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings_test.txt', <http.client.HTTPMessage at 0x7f74e084b750>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBgDKbXivGWZ"
      },
      "source": [
        "train_data = pd.read_table('ratings_train.txt')\n",
        "test_data = pd.read_table('ratings_test.txt')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x95m6L0vKpJ"
      },
      "source": [
        "train_data.drop_duplicates(subset=['document'], inplace=True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYSRX0knvUq7",
        "outputId": "009db4fd-2368-42a3-f52c-01cdbd7043e3"
      },
      "source": [
        "print('총 샘플의 수 :',len(train_data))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플의 수 : 146183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LghFGI3kvgo1",
        "outputId": "abfe7e6e-3330-4645-f268-bc589baafa9a"
      },
      "source": [
        "print(train_data.groupby('label').size().reset_index(name = 'count'))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label  count\n",
            "0      0  73342\n",
            "1      1  72841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5psOtgPvgmF"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "gjfRuJ8BvgjC",
        "outputId": "04726e29-e04d-4052-febd-355293d69b4c"
      },
      "source": [
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "# 한글과 공백을 제외하고 모두 제거\n",
        "train_data[:5]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
              "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0nXm5DFvvcI"
      },
      "source": [
        "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
        "train_data['document'].replace('', np.nan, inplace=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6jmmYviv447"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsIVlgR5v_GI"
      },
      "source": [
        "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
        "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
        "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5j3YRQTv-45"
      },
      "source": [
        "okt = Okt()\n",
        "url = 'https://raw.githubusercontent.com/chaerui7967/stock_predict_news_and_youtube/master/Sentiment_Analysis/data/stopwords_ver1.txt'\n",
        "stopwords = list(pd.read_csv(url, header=None)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdxlCvpgwqOq"
      },
      "source": [
        "X_train = []\n",
        "for sentence in train_data['document']:\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(temp_X)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7_ydAPRwqLO"
      },
      "source": [
        "X_test = []\n",
        "for sentence in test_data['document']:\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(temp_X)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrXze4n8wqF7"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM3izkPDwwgp",
        "outputId": "9ed30a35-80c2-4b13-cf26-0dd3c911c8e2"
      },
      "source": [
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합(vocabulary)의 크기 : 43503\n",
            "등장 빈도가 2번 이하인 희귀 단어의 수: 24308\n",
            "단어 집합에서 희귀 단어의 비율: 55.876606211065905\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 2.093528444535327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhP6jJzfwweN",
        "outputId": "763e0f3e-4a27-458f-a916-55dd8cd106dc"
      },
      "source": [
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기 : 19196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiLUuhQvwwa8"
      },
      "source": [
        "tokenizer = Tokenizer(vocab_size) \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQesvrXJwwYB"
      },
      "source": [
        "y_train = np.array(train_data['label'])\n",
        "y_test = np.array(test_data['label'])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfLjtjK6wv8o"
      },
      "source": [
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmQop78Nxq0b",
        "outputId": "6a65a695-7dca-45e4-93da-6c8a07c9c43c"
      },
      "source": [
        "X_train = np.delete(X_train, drop_train, axis=0)\n",
        "y_train = np.delete(y_train, drop_train, axis=0)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb3E1ISgxqw4"
      },
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j0KaNMIxqtQ",
        "outputId": "3adc83cb-6905-4ea2-b5ea-aba7aebc2656"
      },
      "source": [
        "max_len = 30\n",
        "below_threshold_len(max_len, X_train)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 중 길이가 30 이하인 샘플의 비율: 96.11276230406929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSpAqqdSxwlA"
      },
      "source": [
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZsyLIVVyHUP"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HjidEa4yHRN"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLU8Fl80yHOr"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JVxMFI_yHMc",
        "outputId": "3fd3fe63-6d1a-47c9-dcb6-076899a9705e"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1934/1934 [==============================] - 38s 14ms/step - loss: 0.3928 - acc: 0.8204 - val_loss: 0.3565 - val_acc: 0.8421\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.84205, saving model to best_model.h5\n",
            "Epoch 2/15\n",
            "1934/1934 [==============================] - 27s 14ms/step - loss: 0.3314 - acc: 0.8562 - val_loss: 0.3392 - val_acc: 0.8537\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.84205 to 0.85374, saving model to best_model.h5\n",
            "Epoch 3/15\n",
            "1934/1934 [==============================] - 27s 14ms/step - loss: 0.3067 - acc: 0.8692 - val_loss: 0.3346 - val_acc: 0.8552\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.85374 to 0.85519, saving model to best_model.h5\n",
            "Epoch 4/15\n",
            "1934/1934 [==============================] - 27s 14ms/step - loss: 0.2878 - acc: 0.8793 - val_loss: 0.3330 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.85519 to 0.85908, saving model to best_model.h5\n",
            "Epoch 5/15\n",
            "1934/1934 [==============================] - 26s 13ms/step - loss: 0.2715 - acc: 0.8883 - val_loss: 0.3328 - val_acc: 0.8557\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.85908\n",
            "Epoch 6/15\n",
            "1934/1934 [==============================] - 26s 13ms/step - loss: 0.2567 - acc: 0.8949 - val_loss: 0.3368 - val_acc: 0.8538\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.85908\n",
            "Epoch 7/15\n",
            "1934/1934 [==============================] - 26s 13ms/step - loss: 0.2417 - acc: 0.9028 - val_loss: 0.3482 - val_acc: 0.8519\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.85908\n",
            "Epoch 8/15\n",
            "1934/1934 [==============================] - 26s 13ms/step - loss: 0.2265 - acc: 0.9098 - val_loss: 0.3626 - val_acc: 0.8482\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.85908\n",
            "Epoch 9/15\n",
            "1934/1934 [==============================] - 26s 13ms/step - loss: 0.2106 - acc: 0.9169 - val_loss: 0.3740 - val_acc: 0.8480\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.85908\n",
            "Epoch 00009: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MyM-69GyHFB",
        "outputId": "fb17bf19-3a3e-4a27-b7db-4bf20e3a06b7"
      },
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1527/1527 [==============================] - 8s 5ms/step - loss: 0.3420 - acc: 0.8537\n",
            "\n",
            " 테스트 정확도: 0.8537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4fAJWQNySvi"
      },
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "    score = float(loaded_model.predict(pad_new)) # 예측\n",
        "\n",
        "    if(score > 0.5):\n",
        "        # print(\"{:.2f}% 확률로 긍정 리뷰\\n\".format(score * 100))\n",
        "        return score, 1\n",
        "    elif (score == 0.5):\n",
        "        return score, 0\n",
        "    else:\n",
        "        # print(\"{:.2f}% 확률로 부정 리뷰\\n\".format((1 - score) * 100))\n",
        "        return (1 - score), -1"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDIC7gBUySse"
      },
      "source": [
        "aa = youtube_df.text.iloc[:].values"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uXblQpvySnd"
      },
      "source": [
        "scores = []\n",
        "results = []\n",
        "for i in aa:\n",
        "    score, result = sentiment_predict(i)\n",
        "    scores.append(score)\n",
        "    results.append(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goyHggglySjb",
        "outputId": "d9caec92-f032-4609-a57f-2e76291efe40"
      },
      "source": [
        "np.array(result.reshape(-1,1))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7549134492874146,\n",
              " '매우 늘 증시 마감 상황 정리 해보겠습니다 증권 팀 지수 이 기자나와있습니다네 오늘 대장주 인 삼성전자가 국내에서 갤럭시 s3 출시 에도 불구하고큰 폭으로 하락하면서 우리 시장도 크게 흔들린 하루 였는데 자세한 내용전해주시죠네 오늘 코스피는 삼성전자의 하락 뿐이 그대로 반영된 하루였습니다시총 15% 를 차지하고 있는 삼성전자가 4% 넘게 빠지면서 시장에는 큰충격으로 다가왔습니다 오늘 코스피는 지난 금요일 보다 22.0 1p1.19% 내린 1820 5.3 팔로 마감했습니다외국인이 팔아치운 5000억원 가운데 삼성전자에서 만 3천 400억 원이빠져나갔습니다개인의 5천 5백억 원을 사들였고 기관도 1400억 원 매수 우위를보였습니다외국인 선물 매도 세로 프로그램이 2300억원 매도 우위로 마감의 지수를끌어 내렸습니다업종별로는 삼성전자 여파로 전기전자 업종이 3% 넘게 약세를 보였고 전기가서 화학 등은 소폭 상승 마감했습니다으 우빈이 매물을 쏟아내고 있는 이후 특혜의 삼성전자의 집중 매도 가나오는 이유아무래도 유럽 문제겠죠 네 그렇습니다 가장 큰 원인 유럽 2기 불확실성이투자심리를 위축시키고 있기 때문입니다최근 신흥시장 주시고 우선적으로 팔아 치우고 있는 외국인들이 국내에서가장 많이 보유하고 있는 삼성전자의 종목을 팔아 채우면서 집중 외도 되고있는 상황입니다여기에 삼성전자의 2분기 실적 전당이 기대에 미치지 못할 것이라는 신랑감도 매도세를 부추 겟습니다4 코스피 하락세 와는 달리 이제 코스닥 하락 폭은 크지 않았는데 그이유도 좀 설명해 주시죠 네 상승세로 출발한 코스닥은 낙 판 뒤 쉼부족으로 6거래일 만에 하락했습니다 오늘 코스닥은 지난 금요일 보다0.75 포인트 0.15% 내린 4 184.4 사회 장을 마감했습니다시총 상위종목 가운데 셀트리온의 약세를 보였지만 다음과 서울반도체 cj오쇼핑 등이 상승하는 등 중소형주는 선방 하는 모습을 보였습니다')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-8s3qOi_CJm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}