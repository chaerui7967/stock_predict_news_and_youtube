{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f5f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 유사도 관련 패키지\n",
    "# !pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ffde99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# db 연동\n",
    "import pymysql\n",
    "\n",
    "# 주식 데이터\n",
    "import FinanceDataReader as fdr\n",
    "\n",
    "# nlp\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 유사도 계산\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data(df,com_num = 0):  # panda, numpy, datetime, FinanceDataReader, konlpy, Counter\n",
    "    # 일, 시간 나누기\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    df['date_d'] = df['date'].str[:-2]\n",
    "    df['date_h'] = df['date'].str[-2:]\n",
    "    # 타입을 데이트 타입으로 만듬\n",
    "    df['date_d'] = pd.to_datetime(df['date_d'])\n",
    "    \n",
    "    df = df.sort_values(by='date_d') # 일기준으로 오름차순 정렬\n",
    "    \n",
    "    if com_num != 0:\n",
    "    #     통합 데이터 활용시 업종코드 지정\n",
    "        df = df[df['st_cd'] == com_num] # 해당 회사만 추출\n",
    "        num = str(com_num).zfill(6) # 종목코드를 6자리로 만들어줌\n",
    "        \n",
    "    else:\n",
    "        # 특정 업종만 할 때\n",
    "        num = str(df['st_cd'].iloc[0]).zfill(6)\n",
    "    \n",
    "    # 널값 제거\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # ============== 날짜 조정\n",
    "    ## 전일 15시 ~ 금일 15시로 날짜 조정\n",
    "    after_market = ['15', '16', '17', '18', '19', '20', '21', '22', '23']\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df['date_h'].iloc[i] in after_market:\n",
    "            df['date_d'].iloc[i] += datetime.timedelta(1)\n",
    "    \n",
    "    # 주말 및 공휴일 데이터\n",
    "    ### Holidays\n",
    "    try:\n",
    "        # 서버가 열려있을 때\n",
    "        db = pymysql.connect(user='root',\n",
    "                             passwd='1234',\n",
    "                             host='3.35.70.166',\n",
    "                             db='proj',\n",
    "                             charset='utf8')\n",
    "\n",
    "        cursor = db.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "        sql = \"select * from holidays\"\n",
    "        cursor.execute(sql)\n",
    "        result = cursor.fetchall()\n",
    "    \n",
    "        # DataFrame으로 변경\n",
    "        holi = pd.DataFrame(result)\n",
    "        # db 닫기 --> 안하면 메모리 잡아먹음\n",
    "        db.close()\n",
    "    except:\n",
    "        # 서버 없을 때 깃허브에서 바로 가져옴\n",
    "        db_holi = 'https://raw.githubusercontent.com/chaerui7967/stock_predict_news_and_youtube/master/%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D/data/holidays.csv'\n",
    "        holi = pd.read_csv(db_holi)\n",
    "    \n",
    "    # date 컬럼을 날짜 형식으로 변경\n",
    "    holi['date'] = pd.to_datetime(holi['date'])\n",
    "    \n",
    "    \n",
    "    ### ===================주말 및 공휴일 제외\n",
    "    \n",
    "    ## 주말 및 공휴일만 추출\n",
    "    market_closed = holi[holi['holiday']==\"O\"].reset_index(drop=True)\n",
    "    \n",
    "    ## 휴장일 List 생성\n",
    "    market_closed_list = list(market_closed['date'])\n",
    "    \n",
    "    # 주말 및 공휴일 제외\n",
    "    while len(df[df['date_d'].isin(market_closed_list)]['date_d']) !=0:\n",
    "        for i in df[df['date_d'].isin(market_closed_list)]['date_d'].index:\n",
    "            df['date_d'][i] += datetime.timedelta(1)\n",
    "     \n",
    "    \n",
    "    \n",
    "    # 주식 데이터 가져오기\n",
    "    start_date = str(df['date_d'].iloc[0])[:10].replace('-', '')\n",
    "    end_date = str(df['date_d'].iloc[-1])[:10].replace('-', '')  # 뉴스 데이터 기준 최근 값까지의 주식 데이터\n",
    "    \n",
    "    stock = fdr.DataReader(num, start = start_date, end = end_date)\n",
    "    \n",
    "    # change 컬럼 판단 기준 close에서 수수료 보다 많은 비율이 올랐으면 1, 떨어졌으면 -1, 그 사이 0\n",
    "    point = stock['Close'] * stock['Change']\n",
    "    spli = stock['Close'] * 0.001  # 수수료가 0.1 % 라고 할 때\n",
    "    \n",
    "    stock['ud'] = np.where( point > spli, 1,\n",
    "                          np.where(point < spli, -1, 0))\n",
    "    \n",
    "    stock = stock.reset_index() # 날짜 컬럼 가져오기\n",
    "    \n",
    "    s_1 = stock[['Date','ud']] # 필요한 컬럼만 가져오기\n",
    "    \n",
    "    s_1.columns = ['date_d','ud'] # 컬럼 명 초기화\n",
    "    \n",
    "    # 뉴스데이터와 주식데이터 merge\n",
    "    df_1 = pd.merge(df,s_1,on='date_d')  # 영향받는 컬럼 기준으로 inner 조인 \n",
    "    \n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def giro_dic(df,num = 0):\n",
    "    \n",
    "    df = set_data(df, num)\n",
    "    okt = Okt()\n",
    "    \n",
    "    # 데이터 전처리\n",
    "    # =================== nlp\n",
    "    # 1. 제목, 텍스트 --> 명사, 형용사 추출\n",
    "    # 태그 삭제        \n",
    "    # \\n \\t 삭제\n",
    "    df['text'] = df['text'].str.replace('[\\n|\\t|\\r]','')\n",
    "    df['text'] = df['text'].str.replace('[^a-zA-Z0-9가-힣]','')\n",
    "    \n",
    "    # 토큰화 및 품사 태깅\n",
    "    df['Contents_input'] = df['text'].apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "    \n",
    "    # 명사, 형용사 추출\n",
    "    for i in range(len(df['Contents_input'])):\n",
    "        wordList = []\n",
    "        for j in range(len(df['Contents_input'][i])):\n",
    "            if df['Contents_input'][i][j][1] == 'Noun' or df['Contents_input'][i][j][1] == 'Adjective':\n",
    "                wordList.append(df['Contents_input'][i][j][0])         \n",
    "        df['Contents_input'][i] = wordList\n",
    "    \n",
    "    \n",
    "    # 2. 불용어 처리\n",
    "    # 불용어 사전 로딩\n",
    "    url = 'https://raw.githubusercontent.com/chaerui7967/stock_predict_news_and_youtube/master/%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D/data/stopwords_ver1.txt'\n",
    "    stopwords = list(pd.read_csv(url, header=None)[0])\n",
    "    media = ['매일경제','mkcokr','무단전재및재배포금지','무단전재','재배포금지']\n",
    "    \n",
    "    for i in range(len(df['Contents_input'])):\n",
    "        for word in df['Contents_input'][i]:\n",
    "            if word in stopwords:\n",
    "                df['Contents_input'][i].remove(word)\n",
    "            if word in media:\n",
    "                df['Contents_input'][i].remove(word) \n",
    "    \n",
    "    # 단어길이가 1이하인 단어 제거\n",
    "    for index, wordList in enumerate(df['Contents_input']):\n",
    "        if len(df['Contents_input'][index]) == 0:\n",
    "            continue\n",
    "        for word in wordList:\n",
    "            if len(word) <= 1:\n",
    "                df['Contents_input'][index].remove(word)\n",
    "    \n",
    "    # 주식 감성 사전 로딩 : koself\n",
    "    koself_pro = 'https://raw.githubusercontent.com/chaerui7967/stock_predict_news_and_youtube/master/%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D/data/KOSELF_pos.txt'\n",
    "    koself_neg = 'https://raw.githubusercontent.com/chaerui7967/stock_predict_news_and_youtube/master/%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D/data/KOSELF_neg.txt'\n",
    "    \n",
    "    positive = list(pd.read_csv(koself_pro, header=None)[0])\n",
    "    negative = list(pd.read_csv(koself_neg, header=None)[0])\n",
    "    \n",
    "    # koself 만 쓴 감성점수\n",
    "    df['koself'] = 0\n",
    "    for index, wordList in enumerate(df['Contents_input']):\n",
    "        for word in wordList:\n",
    "            if word in positive:\n",
    "                df['koself'][index] += 1\n",
    "            if word in negative:\n",
    "                df['koself'][index] -= 1\n",
    "                \n",
    "    # 사전 구축\n",
    "    # 유사도 계산으로 긍부정 단어 추가\n",
    "    # 유사도 계산으로 애매한 단어 긍부정 판단--> 긍부정 단어에 다포함되어있는\n",
    "    # 사용 모델 -->  fastText의 이미 학습된 한국어 모델 사용\n",
    "    try:\n",
    "        print(ko_model, '모델 로딩 안해도 됨')\n",
    "    except:\n",
    "        ko_model = models.fasttext.load_facebook_model('cc.ko.300.bin')\n",
    "    \n",
    "    # 단어 수를 늘리기 위해서 유사도 50%이상인 단어를 추가\n",
    "    for i in positive:\n",
    "        pos_si = [j[0] for j in ko_model.wv.similar_by_word(i, 10) if j[1] >= 0.5]\n",
    "    for i in negative:\n",
    "        neg_si = [j[0] for j in ko_model.wv.similar_by_word(i, 10) if j[1] >= 0.5]\n",
    "    \n",
    "    pos_gi = positive + pos_si\n",
    "    neg_gi = negative + neg_si\n",
    "    \n",
    "    \n",
    "    for index, ud in enumerate(df['ud']):\n",
    "        if ud == 1: # positive?\n",
    "            for i in df['Contents_input'][index]:\n",
    "                if (i in pos_gi) and (i in neg_gi):\n",
    "                    # 10개의 유사한 단어에서 positive가 5 이상이면 positive로 판단\n",
    "                    S_word = [j[0] for j in ko_model.wv.similar_by_word(i, 10)]\n",
    "                    word_num = 0\n",
    "                    for word in S_word:\n",
    "                        if word in pos_gi:\n",
    "                            word_num += 1\n",
    "                        if word_num >= 5:\n",
    "                            pos_gi.append(i)\n",
    "                            break\n",
    "                else:\n",
    "                    pos_gi.append(i)\n",
    "                    \n",
    "        elif ud == -1:\n",
    "            for i in df['Contents_input'][index]:\n",
    "                if (i in pos_gi) and (i in neg_gi):\n",
    "                    # 10개의 유사한 단어에서 negative가 5 이상이면 negative로 판단\n",
    "                    S_word = [j[0] for j in ko_model.wv.similar_by_word(i, 10)]\n",
    "                    word_num = 0\n",
    "                    for word in S_word:\n",
    "                        if word in neg_gi:\n",
    "                            word_num += 1\n",
    "                        if word_num >= 5:\n",
    "                            neg_gi.append(i)\n",
    "                            break\n",
    "                else:\n",
    "                    neg_gi.append(i)      \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # 사전 중복 제거\n",
    "    pos_gi = list(set(pos_gi))\n",
    "    neg_gi = list(set(neg_gi))\n",
    "    \n",
    "    # 만들어진 사전으로 점수 도출\n",
    "    df['giro'] = 0\n",
    "    for index, wordList in enumerate(df['Contents_input']):\n",
    "        for word in wordList:\n",
    "            if word in pos_gi:\n",
    "                df['giro'][index] += 1\n",
    "            if word in neg_gi:\n",
    "                df['giro'][index] -= 1\n",
    "            \n",
    "    return df, pos_gi, neg_gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/전처리 전 데이터/df_매일경제_삼성전자.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd01a0-2f53-41ee-b994-5603ddcd37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 모델 크기가 크기 때문에 다른 곳에 저장__git에 커밋되지 않게\n",
    "print(os.getcwd()) # 현재 디렉토리 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b97e8b-952c-4e9d-a25b-969c37220e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../.git/비공개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096184b-c50b-421f-aa2e-c9da229e2c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90e124-d9e1-4a48-8b46-a5dd1838dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이스북 한국어 임베딩 모델 다운 --> 1번만 하면 됨\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "fasttext.util.download_model('ko', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f34363-5a26-434e-befc-cfb8decfcb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_model = models.fasttext.load_facebook_model('cc.ko.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2f420-4167-4c49-9a3d-d0f73f4b6073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prt=df[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b2b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred, pos_gi, neg_gi = giro_dic(prt)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4840d8-5e01-4f0b-a678-d180fa225e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/j/stock_predict/감성분석\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7e5a9-f371-4d51-9008-0478eb89d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "\n",
    "pred.to_csv('pred.csv', index=False)\n",
    "\n",
    "with open('./pos.txt','w') as f:\n",
    "    f.write(pos_gi)\n",
    "with open('./neg.txt', 'w') as f:\n",
    "    f.write(neg_gi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ab715",
   "metadata": {},
   "source": [
    "#### train --> ud 정확도 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc50707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구축된 사전 단어 수\n",
    "len(pos_gi), len(neg_gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9fb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(pred[(pred.koself > 0) & (pred.ud > 0)])+len(pred[(pred.koself < 0) & (pred.ud < 0)])) / (len(pred[pred.ud > 0])+len(pred[pred.ud < 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07141d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(pred[(pred.giro > 0) & (pred.ud >0)]) + len(pred[(pred.giro < 0) & (pred.ud <0)])) / (len(pred[pred.ud > 0])+len(pred[pred.ud < 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c8d51-e0e9-42f6-a853-8f0da2cbd47c",
   "metadata": {},
   "source": [
    "#### test --> 정확도 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d14d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfa7e3-a5af-4f72-99a6-39f7314bee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['giro'] = 0\n",
    "for index, wordList in enumerate(df['Contents_input']):\n",
    "    for word in wordList:\n",
    "        if word in pos_gi:\n",
    "            test['giro'][index] += 1\n",
    "        if word in neg_gi:\n",
    "            test['giro'][index] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3c106-6f4d-466c-90ae-13b0be7837b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test[(test.giro > 0) & (test.ud > 0)])/len(test[test.ud > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95f22d-2904-4759-bf75-e42d4ca8d4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49ff568d",
   "metadata": {},
   "source": [
    "## ---------------- db----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas\n",
    "\n",
    "db = pymysql.connect(\n",
    "    user='root', \n",
    "    passwd='1234', \n",
    "    host='3.35.70.166', \n",
    "    db='proj', \n",
    "    charset = 'utf8'\n",
    ")\n",
    "\n",
    "cursor = db.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "sql = \"select * from news_craw_005930 where length(date) = 10\"  # date 널값 제외\n",
    "cursor.execute(sql)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "# 데이터 프레임으로 변경\n",
    "df = pandas.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c6c3a",
   "metadata": {},
   "source": [
    "### ------------- ver2 실습 -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef255c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbda07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f7f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0b93e48",
   "metadata": {},
   "source": [
    "## --------------------실습 ----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c51e3",
   "metadata": {},
   "source": [
    "## 함수 실습 _ ver1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0979d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_a, pos_cnt, nega_cnt, positive_r, negative_r =  giro_dic(df, 5930)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a317413",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6229378b",
   "metadata": {},
   "source": [
    "## 실습 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5123c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a19647",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].str.replace('[\\n|\\t]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd704a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1f1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22402f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xlrd\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c464859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hundai = pd.read_excel('./news_craw_hyundai.xlsx', engine = 'openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37875b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hundai = df_hundai.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc0563",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hundai['date'] = df_hundai['date'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hundai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hundai.drop('Unnamed: 0',1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d292ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hundai.columns = ['st_n', 'st_cd', 'news', 'n_date', 'title', 'url', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hundai.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f8fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5526740",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nega_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81085d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
