{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"my","language":"python","name":"my"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"colab":{"name":"all_html.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"bdfd0c38"},"source":["import sys, os\n","import requests\n","import selenium\n","from selenium import webdriver\n","import requests\n","from pandas import DataFrame\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import re\n","from datetime import datetime, timedelta\n","import pickle, json, glob, time\n","from tqdm import tqdm\n","\n","###### 날짜 저장 ##########\n","## 현재 시간\n","a=str(datetime.now()-timedelta(days=1))\n","a = a[:a.rfind(':')].replace('-', '.')\n","a=a[:a.find(' ')]\n","a\n","\n","sleep_sec = 0.3\n","\n","#######################<  함수 파트 >##############################\n","\n","#####################################################################\n","################################################################  html 전체 전처리  #####################################\n","def cleanhtml(raw_html):\n","    try:\n","        cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n","        cleantext = re.sub(cleanr, '', raw_html)\n","        cleantext = cleantext.replace('\\n','')\n","        cleantext = cleantext.replace('\\t','')\n","        cleantext=cleantext[cleantext.find('통합검색 검색닫기'):cleantext.find('재배포 금지')]\n","    except:\n","        cleantext = '결과없음'\n","    print('html 클린')\n","    return cleantext\n","\n","#####\n","################################################# 한 페이지 전체 읽기 ############################################################\n","def one_page_craw(url,query,cd,press_nm):\n","    browser.get(url)\n","    time.sleep(0.07)\n","    df_one_page_craw = pd.DataFrame()\n","    \n","    #페이지 수\n","#     page_ct=len(abc.find_elements_by_xpath('.//a'))\n","#     page_ct_tg = abc.find_elements_by_xpath('.//a')\n","\n","# ####동적 제어로 페이지 넘어가며 크롤링\n","    news_dict = {}\n","    idx = 1\n","    cur_page = 1\n","    table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n","    li_list = table.find_elements_by_xpath('./li[contains(@id, \"sp_nws\")]')\n","    news_num = len(li_list)\n","    news_df = pd.DataFrame()\n","\n","    while idx < news_num:\n","        table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n","        li_list = table.find_elements_by_xpath('./li[contains(@id, \"sp_nws\")]')\n","        area_list = [li.find_element_by_xpath('.//div[@class=\"news_area\"]') for li in li_list]\n","        a_list = [area.find_element_by_xpath('.//a[@class=\"news_tit\"]') for area in area_list]\n","\n","        for n in a_list[:min(len(a_list), news_num-idx+1)]:\n","            n_url = n.get_attribute('href')\n","\n","            news_sss = {'st_n':query,\n","                              'st_cd':cd,\n","                            'news': press_nm,\n","                            'date':str(crawling_main_text(n_url)[1]),\n","                              'title' : n.get_attribute('title'), \n","                              'url' : n_url,\n","                              'text' : crawling_main_text(n_url)[0]}\n","            idx += 1\n","#             pbar.update(1)\n","\n","            news_df=news_df.append(news_sss,ignore_index=True)\n","            \n","    return news_df\n","\n","\n","\n","########################################### 언론사별 본문 위치 태그 파싱 함수 ############################################\n","print('본문 크롤링에 필요한 함수를 로딩하고 있습니다...\\n' + '-' * 100)\n","def crawling_main_text(url):\n","\n","    req = requests.get(url)\n","    req.encoding = None\n","    soup = BeautifulSoup(req.text, 'html.parser')\n","    dates=''\n","    res = []\n","    \n","    # 연합뉴스\n","    if ('://yna' in url) | ('app.yonhapnews' in url): \n","        main_article = soup.find('div', {'class':'story-news article'})\n","        if main_article == None:\n","            main_article = soup.find('div', {'class' : 'article-txt'})\n","            \n","        text = main_article.text\n","        \n","   ###### 전체 html로 수정 ######     \n","   # 매일경제, req.encoding = None 설정 필요\n","    elif 'mk.co' in url:\n","        try:\n","            text = soup.find('div', {'class' : 'art_txt'}).text\n","            text2 = soup.find('li', {'class' : 'lasttime'}).text\n","            dates = re.sub(r'[^0-9]', '', text2)\n","        except:\n","            try:\n","                text = soup.find('div', {'class' : 'view_txt'}).text\n","            except:\n","                text=cleanhtml(str(soup))\n","            try:\n","                text2 = soup.find('li', {'class' : 'lasttime'}).text\n","                dates = re.sub(r'[^0-9]', '', text2)\n","            except:\n","                text2 = '0'\n","#             dates = re.sub(r'[^0-9]', '', text2)\n","\n","    # 매일경제, req.encoding = None 설정 필요\n","#     elif 'mk.co' in url:\n","#         text=cleanhtml(str(soup))\n","#         try:\n","#             text2 = soup.find('li', {'class' : 'lasttime'}).text\n","#         except:\n","#             text2 = 'None'\n","#         dates = re.sub(r'[^0-9]', '', text2)\n","#         dates = str(dates)\n","        \n","    # 그 외\n","    else:\n","        text == None\n","    ###### 전체 html로 수정 ######\n","#     try:\n","#         text=text.replace('\\n','').replace('\\r','').replace('<br>','').replace('\\t','')\n","#     except:\n","#         text= None\n","    \n","    res.append(text)\n","    res.append(dates[:10])\n","    return res\n","    \n","    \n","press_nm = '매일경제'\n","querys= {'하이닉스':'000660'}\n","print('검색할 언론사 : {}'.format(press_nm))\n","\n","#######################<  실행 파트 >##############################\n","#########################################################################################################################\n","################### 브라우저를 켜고 검색 키워드 입력 ###################################################################\n","# querys = {\"삼성전자\":'005930','하이닉스':'000660','네이버':'035420','카카오':'035720','삼성바이오로직스':'207940',\n","#           'LG화학':'051910','현대차':'005380','셀트리온':'068270'}\n","# 'SDI':'006400'\n","querys= {\"삼성전자\":'005930'}\n","\n","# query = input('검색할 키워드  : ')\n","# news_num = int(input('수집 뉴스의 수(숫자만 입력) : '))\n","# 데이터 프레임\n","news_df = DataFrame(columns=['st_n','st_cd','news','date','title','url','text'])\n","\n","for query,cd in querys.items():\n","    print(' query,cd',query,cd)\n","    # 1 페이지\n","    cureent_nm = 1\n","#     news_num=16\n","    press_nm = '매일경제'\n","    print('\\n' + '=' * 100 + '\\n')\n","\n","    print('브라우저를 실행시킵니다(자동 제어)\\n')\n","    chrome_options = webdriver.ChromeOptions()\n","    ############# DevToolsActivePort file doesn't exist error 해결법 #############\n","    chrome_options.add_argument('--headless')\n","    chrome_options.add_argument('--no-sandbox')\n","    chrome_options.add_argument(\"--single-process\")\n","    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n","\n","    chrome_path = './data/chromedriver'\n","    browser = webdriver.Chrome(chrome_path,chrome_options=chrome_options)\n","\n","    news_url = 'https://search.naver.com/search.naver?where=news&query={}&sm=tab_opt&sort=0&photo=0& \\\n","        field=0&pd=3&ds={}&de={}&start=1'.format(query,20200101,a)\n","    browser.get(news_url)\n","    time.sleep(sleep_sec)\n","\n","\n","    ######### 언론사 선택 및 confirm #####################\n","    print('설정한 언론사를 선택합니다.\\n')\n","\n","#     search_opn_btn = browser.find_element_by_xpath('//a[@class=\"btn_option _search_option_open_btn\"]')\n","#     search_opn_btn.click()\n","#     time.sleep(sleep_sec)\n","\n","    bx_press = browser.find_element_by_xpath('//div[@role=\"listbox\" and @class=\"api_group_option_sort _search_option_detail_wrap\"]//li[@class=\"bx press\"]')\n","\n","    # 기준 두번 째(언론사 분류순) 클릭하고 오픈하기\n","    press_tablist = bx_press.find_elements_by_xpath('.//div[@role=\"tablist\" and @class=\"option\"]/a')\n","    press_tablist[1].click()\n","    time.sleep(sleep_sec)\n","\n","    # 첫 번째 것(언론사 분류선택)\n","    bx_group = bx_press.find_elements_by_xpath('.//div[@class=\"api_select_option type_group _category_select_layer\"]/div[@class=\"select_wrap _root\"]')[0]\n","\n","    press_kind_bx = bx_group.find_elements_by_xpath('.//div[@class=\"group_select _list_root\"]')[0]\n","    press_kind_btn_list = press_kind_bx.find_elements_by_xpath('.//ul[@role=\"tablist\" and @class=\"lst_item _ul\"]/li/a')\n","\n","    \n","###########################  언로사 클릭 ###########################################################################\n","    for press_kind_btn in press_kind_btn_list:\n","\n","        # 언론사 종류를 순차적으로 클릭(좌측)\n","        press_kind_btn.click()\n","        time.sleep(sleep_sec)\n","\n","        # 언론사선택(우측)\n","        press_slct_bx = bx_group.find_elements_by_xpath('.//div[@class=\"group_select _list_root\"]')[1]\n","        # 언론사 선택할 수 있는 클릭 버튼\n","        press_slct_btn_list = press_slct_bx.find_elements_by_xpath('.//ul[@role=\"tablist\" and @class=\"lst_item _ul\"]/li/a')\n","        # 언론사 이름들 추출\n","        press_slct_btn_list_nm = [psl.text for psl in press_slct_btn_list]\n","\n","        # 언론사 이름 : 언론사 클릭 버튼 인 딕셔너리 생성\n","        press_slct_btn_dict = dict(zip(press_slct_btn_list_nm, press_slct_btn_list))\n","\n","        # 원하는 언론사가 해당 이름 안에 있는 경우\n","        # 1) 클릭하고\n","        # 2) 더이상 언론사분류선택 탐색 중지\n","        if press_nm in press_slct_btn_dict.keys():\n","            print('<{}> 카테고리에서 <{}>를 찾았으므로 탐색을 종료합니다'.format(press_kind_btn.text, press_nm))\n","\n","            press_slct_btn_dict[press_nm].click()\n","            time.sleep(sleep_sec)\n","\n","            break\n","\n","    ################ 뉴스 크롤링 ########################\n","    \n","    \n","    print('\\n크롤링을 시작합니다.')\n","    # ####동적 제어로 페이지 넘어가며 크롤링\n","    news_dict = {}\n","    idx = 1\n","    cur_page = 1\n","    \n","    news_dict = {}\n","    idx = 1\n","    cur_page = 1\n","    table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n","    li_list = table.find_elements_by_xpath('.//a')\n","    # news_num은 10\n","    news_numss = len(li_list)\n","    abc = browser.find_element_by_xpath('//div[@class=\"sc_page_inner\"]')\n","    \n","    #페이지 수\n","    page_ct=len(abc.find_elements_by_xpath('.//a'))\n","    page_ct_tg = abc.find_elements_by_xpath('.//a')\n","    \n","    ################# 한 페이지 전체 읽기 #######################\n","    urls=browser.current_url\n","    res=one_page_craw(urls,query,cd,press_nm)\n","    news_df=news_df.append(res,ignore_index=True)\n","    ############### 다음 페이지 넘기기 ############################\n","    btn_next=browser.find_element_by_xpath('//a[@role=\"button\" and @class=\"btn_next\"]').get_attribute('href')\n","    print(btn_next)\n","    while btn_next != None:\n","        btn_next=browser.find_element_by_xpath('//a[@role=\"button\" and @class=\"btn_next\"]').get_attribute('href')\n","#     while page_ct >cureent_nm:\n","        print('cureent_nm-----',cureent_nm)\n","        browser.find_element_by_xpath('//a[@role=\"button\" and @class=\"btn_next\"]').click()\n","        nxt_pg = browser.current_url\n","#         nxt_pg= browser.current_url+'&start='+str(cureent_nm*10+1)\n","        #### 다음 페이지 이동 ####\n","        browser.get(nxt_pg)\n","        time.sleep(sleep_sec)\n","        urls=browser.current_url\n","        res=one_page_craw(urls,query,cd,press_nm)\n","        news_df=news_df.append(res,ignore_index=True)\n","        cureent_nm += 1\n","        \n","#         pages = browser.find_element_by_xpath('//div[@class=\"sc_page_inner\"]')\n","#         next_page_url = [p for p in pages.find_elements_by_xpath('.//a') if p.text == str(cur_page)][0].get_attribute('href')\n","\n","#         browser.get(next_page_url)\n","#         time.sleep(sleep_sec)\n","    ################################################################\n","    else:\n","        print('\\n브라우저를 종료합니다.\\n' + '=' * 100)\n","        time.sleep(0.3)\n","    \n","browser.close()\n","\n","#     browser.close()\n","#     break\n","#### 데이터 전처리하기 ###################################################### \n","\n","print('데이터프레임 변환\\n')\n","# news_df = DataFrame(news_dict).T\n","\n","# folder_path = os.getcwd()\n","# xlsx_file_name = '네이버뉴스_본문_{}개.xlsx'.format('삼성전자')\n","\n","# news_df.to_excel(xlsx_file_name)\n","\n","# print('엑셀 저장 완료 | 경로 : {}\\\\{}\\n'.format(folder_path, xlsx_file_name))\n","\n","# os.startfile(folder_path)\n","\n","print('=' * 100 + '\\n결과물의 일부')\n","\n","\n","\n","# news_df\n","\n","########################################## DB에 데이터 넣기 ################################################\n","import pymysql\n","import warnings\n","import MySQLdb\n","from sqlalchemy import create_engine\n","import sqlalchemy\n","warnings.filterwarnings(action='ignore')\n","\n","pymysql.install_as_MySQLdb()\n","\n","engine = create_engine(\"mysql+mysqldb://root:\"+\"1234\"+\"@3.35.70.166/proj\", encoding='utf8')\n","conn = engine.connect()\n","\n","news_df.to_sql(name='news_craw_test', con=engine, if_exists='append',index = False,dtype = {\n","'st_n':sqlalchemy.types.VARCHAR(10),\n","'st_cd':sqlalchemy.types.VARCHAR(10),\n","'news': sqlalchemy.types.TEXT(),\n","'n_date':sqlalchemy.types.VARCHAR(10),\n","'title' : sqlalchemy.types.TEXT(), \n","'url' :sqlalchemy.types.TEXT(),\n","'text' : sqlalchemy.types.TEXT()\n","\n","})\n","\n","conn.close()\n"],"id":"bdfd0c38","execution_count":null,"outputs":[]}]}